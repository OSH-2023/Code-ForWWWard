# 调研报告

- [调研报告](#调研报告)
  - [小组成员](#小组成员)
  - [项目背景](#项目背景)
      - [时代背景：无人驾驭安全性问题](#时代背景无人驾驭安全性问题)
      - [发展现状](#发展现状)
      - [操作系统实时性问题](#操作系统实时性问题)
      - [ROS的具体任务](#ros的具体任务)
  - [立项依据及理论知识](#立项依据及理论知识)
    - [ROS2](#ros2)
      - [ROS开发环境](#ros开发环境)
      - [ROS2中的重要概念](#ros2中的重要概念)
      - [ROS2的通信机制](#ros2的通信机制)
      - [ROS2与ROS1的区别](#ros2与ros1的区别)
    - [cache](#cache)
    - [DMA](#dma)
    - [超低延迟](#超低延迟)
  - [运行环境](#运行环境)
    - [QEMU](#qemu)
      - [What is QEMU](#what-is-qemu)
  - [重要性分析](#重要性分析)
  - [相关项目](#相关项目)
  - [参考文献](#参考文献)

## 小组成员

* 王润泽
* 王道宇
* 封霁芩
* 陈应豪
* 王昱

## 项目背景

#### 时代背景：无人驾驭安全性问题

- 近年来，无人驾驶事故频发，安全性问题已经成为了制约无人驾驶大规模铺设的瓶颈之一。这里有一个典型的例子是：特斯拉在2020年6月台北仙桃发生的事故，车辆在开启自动驾驶的状态下毫无减速的撞上卡车。
  
  ![](https://mp.ofweek.com/Upload/News/Img/member4608/202006/wx_article_20200601221123_1FQoVh.jpg)
  
  人身安全至关重要，正因为如此，无人车驾驶的安全问题理应排在首位。要解决像上述的安全问题，一个非常重要的方向就是提高操作系统的实时性。

#### 发展现状

- **百度Apollo**
  
  > 百度Apollo是百度公司推出的自动驾驶开放平台。该平台具备包括感知、定位、规划、控制等多个模块，可支持L2~L4级别的自动驾驶功能，同时，百度还提供了基于Apollo平台的自动驾驶解决方案，如Apollo Go Robotaxi等，已经在多个城市进行了测试和落地应用。
  
  Apollo平台基于ROS开发，但是对通信机制部分进行了众多改变，使得Apollo平台的实时性得到了大幅提升。Apollo平台的实时性优化主要体现在以下几个方面：
  
  - **去中心化**，也就是去掉ROS Master，这部分使用了DDS技术。
      ![img](http://www.uml.org.cn/ai/images/2018052844.jpg)
  - 使用**共享内存**的方法，优化大数据传输的瓶颈。
      ![img](http://www.uml.org.cn/ai/images/2018052845.jpg)
  - 使用Protobuf优化数据格式的兼容性。
      ![img](http://www.uml.org.cn/ai/images/2018052846.jpg)

#### 操作系统实时性问题

- 目前计算机主流的方式是将文件读入后先拷贝到内存中，再通过数次拷贝进入cache。等到文件处理完成后，依然需要经过多次拷贝。通过DMA技术和局部性原理，可实现数据0拷贝，降低数据传输消耗的时间，以提升操作系统的实时性。

#### ROS的具体任务

- 编写ROS高实时模块与相关应用程序对接，降低数据传输延迟

## 立项依据及理论知识

### ROS2

* `ROS`(`Robot Operating System`，机器人操作系统)[^4]，是专为机器人软件开发所设计出来的一套电脑操作系统架构。它提供类似于操作系统的服务，包括硬件抽象描述、底层驱动程序管理、共用功能的执行、程序间消息传递、程序发行包管理，它也提供一些工具和库用于获取、建立、编写和执行多机融合的程序。事实上，`ROS`并不是一个操作系统，而是编写机器人软件程序的一种具有高度灵活性的分布式软件架构。`ROS`可以分成两层：低层是操作系统层，高层是各类软件包。

#### ROS开发环境

- **gazebo**：一款免费的机器人仿真软件，提供高保真度的物理模拟，一整套传感器模型，以及对用户和程序非常友好的交互方式。能够在复杂的室内和室外环境中准确高效地模拟机器人工作的功能，通常与ROS联合使用，为开发者提供了优异的仿真环境。

#### ROS2中的重要概念

* **节点**：每个节点负责单个模块，**参数**是节点的配置。 
  * `ROS2`与`ROS`比较:`ROS`通过一个核心的`master`节点管理所有节点的通讯 ，如果`master`节点崩溃整个系统会出错；`ROS2`引入了`DDS`(`data distribution service`，数据分发服务)，各个节点可以通过`DDS`进行通信，可以**一对一、一对多、多对多**互相通信，增强了安全性、可靠性，同时提高了实时性[^5]。

![](http://dev.ros2.fishros.com/doc/_images/Nodes-TopicandService.gif)

* **话题**：作用是充当节点之间交换信息的总线，它是基于发布者-订阅者模型
  * 话题可以是**一对多、多对一、多对多**的(见下图)

![](http://dev.ros2.fishros.com/doc/_images/Topic-MultiplePublisherandMultipleSubscriber.gif)

* **服务**：服务是基于调用和响应模型的，它会向具体调用的客户端传递数据
  * 一个服务只能有一个服务端，但可以有多个客户端(见下图)

![](http://dev.ros2.fishros.com/doc/_images/Service-MultipleServiceClient.gif)

* **动作**：是由**目标、反馈、结果**三部分组成，是建立在话题、服务之上的。它基于客户端-服务器模型，客户端`NODE`向服务端`NODE`发送**目标**，服务端`NODE`返回**反馈流、结果**
  * 动作与服务类似，但是动作是**可抢占的**。

![](http://dev.ros2.fishros.com/doc/_images/Action-SingleActionClient.gif)

#### ROS2的通信机制

* ***Publish-Subscribe(1)***[^6]
  * ***Pub***通过Topic向***Sub***传递数据
  * 该机制常用于连续数据流，数据可以在任何时间独立于任何***Pub***/***Sub***发布和订阅
* ***Service-Client(2)***
  * ***Client***向***Server***发出请求，***Server***执行任务后将response返回给***Client***
  * 该机制常用于快速终止的远程过程调用，不应该用于运行时间较长的进程，特别是，如果发生特殊情况，可能需要抢占的过程。并且它们永远不应改变或依赖于状态，以避免对其他节点产生不必要的副作用
* ***Actions(3)***
  * 基于***Service-Client***，在执行任务时产生了多次Fback
  * 该机制常用于移动机器人或**运行更长时间但在执行过程中提供反馈的任何离散行为**

![](research/image/ROS2通信.png)

#### ROS2与ROS1的区别

* ROS1的不足
  * 仅支持Linux系统
  * 基于TCP/IP的通信方式，点对点的分布式通信机制，实时性差，系统开销大
  * 依赖于ROS master管理所有节点的通讯，稳定性差
  * 对Python3支持不友好
* ROS2的改进
  * 支持Linux、Windows、Mac OS、嵌入式RTOS等，跨平台特性得到改善
  * 引入DDS通信协议，可通过零拷贝的方式传递消息，节约CPU、内存的资源，实时性得到了很大的提升
    * DDS（Data Distribution Service）是一种面向实时通信的消息传递中间件技术，它允许不同设备之间进行实时的、可靠的数据交换。DDS旨在解决数据分发和通信问题，它支持多种编程语言和平台，并提供了高效、安全、可靠的数据交换方式。
    * DDS基于发布-订阅模式（Publish-Subscribe），允许多个发送者（Publishers）同时向多个接收者（Subscribers）广播数据，且能够自动进行数据交换的协商和管理。DDS可以提供实时性能，支持大量的数据量，同时具备高可靠性和安全性。
    * DDS与其他通信技术（如消息队列、WebSocket等）相比，具有更高的实时性、更可靠的通信、更强大的管理和更灵活的数据模型等优势，因此被广泛应用于需要高效实时通信的领域，如工业自动化、航空航天、医疗、智能交通等。
  * 去中心化：去除了ROS master中心节点管理器，节点地位平等，可以**一对一、一对多、多对多**互相通信，稳定性、可靠性得到了很大的提高
  * 支持Python3

### cache

- **程序如何运行？**
  
    程序是运行在`RAM`中的，`RAM`就是我们常说的`DDR`（`DDR3`、`DDR4`、`DDR5`），我们称之为主存`main memory`。当我们需要运行一个进程的时候，首先会从磁盘设备（例如，`eMMC`、`UFS`、`SSD`等）中将可执行程序`load`到主存中，然后开始执行。在`CPU`内部存在一堆的通用寄存器（`register`）。
  
    大概过程类似这样[^8]：
  
    ![](research/image/4%E6%9C%883%E6%97%A5%E7%8E%8B%E9%81%93%E5%AE%87_pic/4%E6%9C%883%E6%97%A5%E5%9B%BE%E7%89%871.jpg)
  
    但是由于`CPU`通用寄存器的速度和主存之间存在着太大的差异。两者之间的速度大致如下关系：
  
    ![](https://github.com/OSH-2023/forwwward/blob/main/docs/research/image/4%E6%9C%883%E6%97%A5%E7%8E%8B%E9%81%93%E5%AE%87_pic/4%E6%9C%883%E6%97%A5%E5%9B%BE%E7%89%872.jpg)
  
    在速度差了百倍的情况下，`CPU`对主存的`load/store`操作实际上都非常慢。但是试图提升主存的速度，又要考虑到现今主存动不动几个`GB`的容量，**成本可能过高**。因此产生了一个折中的办法，那就是制作一块速度极快但是容量极小的存储设备。那么其成本也不会太高，我们称之为`cache memory`，在硬件上我们将`cache`放置在`CPU`和主存**之间**，作为**主存数据**的缓存。
  
    当`CPU`试图从主存中`load/store`数据的时候， `CPU`会首先从`cache`中查找对应地址的数据是否缓存在`cache` 中。如果是，直接从`cache`中拿到数据并返回给`CPU`。当存在`cache`的时候，程序运行的流程将会变成如下:
  
    ![](https://github.com/OSH-2023/forwwward/blob/main/docs/research/image/4%E6%9C%883%E6%97%A5%E7%8E%8B%E9%81%93%E5%AE%87_pic/4%E6%9C%883%E6%97%A5%E5%9B%BE%E7%89%873.jpg)
  
    ***CPU和主存之间直接数据传输的方式转变成CPU和cache之间直接数据传输。cache负责和主存之间数据传输。***
  
  - 关于多级`cache`
    
    一级`cache`并没有满足人们对性能的要求，当cache中没有缓存我们想要的数据的时候，依然需要漫长的等待从主存中load数据。所以我们引入了三级`cache`，在刚刚提到的`L1 cache`之后再接上`L2 cache`，在`L2 cache`和主存之间再连接`L3 cache`。等级越高，速度越慢，容量越大。但是速度相比较主存而言，依然很快。不同等级`cache`速度之间关系如下：
    
    ![](https://github.com/OSH-2023/forwwward/blob/main/docs/research/image/4%E6%9C%883%E6%97%A5%E7%8E%8B%E9%81%93%E5%AE%87_pic/4%E6%9C%883%E6%97%A5%E5%9B%BE%E7%89%874.jpg)

- **cache**的部分操作
  
  - 外部设备I/O和DMA传输[^9]
    例如，在博通机顶盒平台中，内存加解密在单独的安全芯片中进行，安全芯片访问的数据通过**DMA**进行传输操作。因此，在进行内存加解密前，需要**flush D-Cache**操作将数据同步到到内存中供安全芯片访问；加解密完成后需要执行**invalidate D-Cache**操作，以确保CPU访问的数据是安全芯片加解密的结果，而不是Cache之前保存的数据。
  
  - 存取数据
    
    通常Cache分为I-Cache和D-Cache，取指令时访问I-Cache，读写数据时访问D-Cache。如果一段代码保存在外设（如nand flash或硬盘）上，CPU想执行这段代码，需要先将这段代码作为数据复制到内存再将这段代码作为指令执行。由于写入数据和读取指令分别通过D-Cache和I-Cache，所以需要同步D-Cache和I-Cache，即复制 后需要先将D-Cache写回到内存，而且还需要作废当前的I-Cache以确保执行的是Memory内更新的代码，而不是I-Cache中缓存的数据。
  
  - **cache**存储计算
    
    CPU和cache以字为单位进行数据交换，而cache却是以块为单位进行数据交换的。在cache中划分若干个字为一行，在内存中划分若干个字为一块，这里的行和块是大小相等的。
    
    每个缓存里面都是由缓存行组成的，缓存系统中是以缓存行（``cache line``）为单位存储的。缓存行是2的整数幂个连续字节，一般为32-256个字节。最常见的缓存行大小是64个字节。当多线程修改互相独立的变量时，如果这些变量共享同一个缓存行，就会无意中影响彼此的性能，这就是``伪共享``。缓存行上的写竞争是运行在SMP系统中并行线程实现可伸缩性最重要的限制因素。
    
    在x86架构中，cache与内存之间采用了组相联映射方式（Set Associativity），具体的方法是先将cache的行进行分组，然后内存块按照组号求模来决定该内存块放置到cache的哪一个组。但是具体放置在组内哪一行都可以，具体由cache替换算法（先进先出/非最近使用/最久未使用）决定。这种映射方式既易于CPU查询cache是否命中，也避免了cache与内存进行频繁数据交换。
    
    > x86中`prefetch`指令可实现将数据从内存传输至cache
  
  - 关于缓存行伪共享的补充
    
    ``cache line``的状态有4个：
    
    - **Invalid**，表明该``cache line``已失效，它要么已经不在cache中，要么它的内容已经过时。处于该状态下的``cache line``等同于它从来没被加载到cache中。
    
    - **Shared**，表明该``cache line``是内存中某一段数据的拷贝，处于该状态下的``cache line``只能被cpu读取，不能写入，因为此时还没有独占。不同cpu的cache line都可以拥有这段内存数据的拷贝。
    
    - **Exclusive**，和 Shared 状态一样，表明该``cache line``是内存中某一段数据的拷贝。区别在于，该``cache line``独占该内存地址，其他处理器的cache line不能同时持有它，如果其他处理器原本也持有同一``cache line``，那么它会马上变成“Invalid”状态。
    
    - **Modified**，表明该``cache line``已经被修改，``cache line``只有处于Exclusive状态才能被修改。此外，已修改``cache line``如果被丢弃或标记为Invalid，那么先要把它的内容回写到内存中。
    
    cpu有读取数据的动作，有独占的动作，有独占后更新数据的动作，有更新数据之后回写内存的动作，根据”窥探协议“的规范，每个动作都需要通知到其他cpu，于是有以下的消息机制：
    
    - **Read**，cpu发起读取数据请求，请求中包含需要读取的数据地址。
    - **Read Response**，作为Read消息的响应，该消息可能是内存响应的，也可能是某cpu响应的(比如该地址在某cpu ``cache Line``中为Modified状态，则该cpu必须返回该地址的最新数据)。
    - **Invalidate**，cpu发起”我要独占一个``cache line``，其他cpu请失效对应的cache line“的消息，消息中包含了内存地址，所有的其它cpu需要将对应cache line置为Invalid状态。
    - **Invalidate ACK**，收到Invalidate消息的cpu在将对应cache line置为Invalid后，返回Invalid ACK。
      Read Invalidate，相当于Read消息+Invalidate消息，即取得数据并且独占它，将收到一个Read Response和所有其它cpu的Invalidate ACK。
    - **Write back**，写回消息，即将状态为Modified的cache line写回到内存，通常在该行将被替换时使用。现代cpu cache基本都采用”写回(Write Back)”而非”直写(Write Through)”的方式。

### DMA

- **基本定义**：DMA[^7]，全称Direct Memory Access，即直接存储器访问。DMA技术通过直接访问系统内存，实现设备和系统之间的数据传输，可以减轻CPU的负担，提高数据传输的效率[^2]。
  
  > DMA传输将数据从一个地址空间复制到另一个地址空间，提供在外设和存储器之间或者存储器和存储器之间的高速数据传输。当CPU初始化这个传输动作，传输动作本身是由DMA控制器来实现和完成的。DMA传输方式无需CPU直接控制传输，也没有中断处理方式那样保留现场和恢复现场过程，通过硬件为RAM和IO设备开辟一条直接传输数据的通道，使得CPU的效率大大提高。

- **总线控制**：CPU和外设再写入系统内存时，可能发生总线争用，有以下三种方式处理冲突：
  
  - **突发模式（Burst mode）**：在突发模式下，整个数据块以一个连续的序列传输。一旦 DMA 控制器被 CPU 授予对系统总线的访问权限，它就会在将系统总线的控制权交还给 CPU 之前传输数据块中的所有字节数据，但会使 CPU 在相对较长的时间内处于非活动状态。
  - **循环窃取模式（Cycle stealing mode）**：循环窃取模式用于 CPU 不应在突发传输模式所需的时间长度内被禁用的系统。在循环窃取模式下，通过不断获得和释放对系统总线的控制，DMA 控制器实质上交织了指令和数据传输。CPU 处理一条指令，然后 DMA 控制器传输一个数据值，依此类推。数据传输速度没有那么快，但 CPU 的空闲时间没有突发模式那么长。
  - **透明模式（Transparent mode）**：透明模式需要最多的时间来传输数据块，但就整体系统性能而言，它也是最有效的模式。在透明模式下，DMA 控制器仅在 CPU 执行不使用系统总线的操作时传输数据。透明模式的主要优点是 CPU 永远不会停止执行其程序并且 DMA 传输在时间上是免费的，而缺点是硬件需要确定 CPU 何时不使用系统总线，这可能很复杂. 

- **缓存一致性（Cache coherency）**
  
  ![DMA 导致缓存不一致](https://upload.wikimedia.org/wikipedia/commons/thumb/4/49/Cache_incoherence_write.svg/559px-Cache_incoherence_write.svg.png)
  
  > 当设备向内存写入数据时，由于CPU可能会将数据缓存在L1、L2、L3缓存中，设备无法直接访问缓存，因此会向内存中写入数据，但是缓存中的数据可能已经发生了改变。同样地，当设备从内存中读取数据时，由于CPU可能将数据缓存在缓存中，设备读取的数据可能是缓存中的数据，而不是最新的数据。这就是所谓的缓存一致性问题。
  > 为了解决缓存一致性问题，现代的CPU和设备通常采用了一些技术来保证缓存一致性。例如，CPU可以使用cache一致性协议（如MESI）来保证多个缓存之间的一致性。当设备向内存写入数据时，缓存中对应的数据会被标记为“失效”，即无效状态，这样在其他缓存中的数据就会被清除。当CPU访问该内存地址时，会从内存中重新读取最新的数据，从而保证缓存一致性。
  > DMA控制器通常也采用类似的机制来保证缓存一致性。在进行DMA数据传输时，DMA控制器会通知CPU缓存该内存地址中的数据已经失效，以便CPU在下次访问该内存地址时能够从内存中读取最新的数据，而不是从缓存中读取旧数据。一些现代的DMA控制器甚至支持cache一致性协议，从而可以直接与CPU的缓存进行通信，提高了系统的性能和可靠性。
  
  - [MESI协议](https://zhuanlan.zhihu.com/p/123926004)
    所有cache与内存，cache与cache（cache之间也会有数据传输）之间的传输都发生在一条共享的总线上，而所有的cpu都能看到这条总线，同一个指令周期中，只有一个cache可以读写内存，所有的内存访问都要经过仲裁（arbitrate）。
  - MESI协议的思想是，cahce不但与内存通信时和总线打交道，而且它会不停地窥探总线上发生的数据交换，跟踪其他cache在做什么。所以当一个cache代表它所属的cpu去读写内存时，其它cpu都会得到通知，它们以此来使自己的cache保持同步。

- **DMA传输模式**[^11]
  
  - 外设到存储器
  
  - 存储器到外设
  
  - 存储器到存储器

### 超低延迟

![](https://github.com/OSH-2023/forwwward/blob/main/docs/research/image/Datapath.png)

- 传统意义下，在数据包从接收、处理到发送的整个流程中，需要经历上述图片所示的几个流程[^10]

> 1. 客户端以TCP/IP数据包的形式发送请求，服务器通过网络接口卡(NIC)接收这些数据包。
> 
> 2. 服务器处理TCP/IP数据包，并将负载路由到指定的应用程序。此处理包括涉及TCP/IP堆栈的协议计算、数据包描述符和有效负载移动的多个服务器内存访问以及各种其他系统开销活动(例如，中断处理和缓冲区管理)。
> 
> 3. 应用程序确认客户端请求，并确认它需要来自存储的数据来响应请求。
> 
> 4. 应用程序访问存储以获取满足客户机请求所需的数据。
> 
> 5. 存储将请求的数据返回给应用程序。
> 
> 6. 应用程序使用从存储器接收的附加数据完成对客户机请求的处理。
> 
> 7. 服务器通过网络连接将响应路由回去，以TCP/IP包的形式发送到客户端。

- 但是这种传输方式，对于当今的需求，其结果是显著的系统开销、过多的内存访问和低效的TCP/IP处理。可以预见的是，当处理的信息文件较小的时候，系统开销就成为了I/O瓶颈。

- 传统意义下的低延迟通常在500ms以下，但是超低延迟显然与低延迟有很大差异

- github平台上的低延迟更多是通过对网络流协议的设计来实现，没有涉及对于内存拷贝等系统开销的处理

- intel的调研报告中给出了一个超低延迟的处理方案，一共分为4个部分
  
  > - **Parallel Processing of TCP and Memory Functions.** Lowers  system overhead and improves the efficiency of TCP stack  processing by using the abilities of the CPU to **execute multiple instructions per clock**, **pre-fetch TCP/IP header information into cache**, and **perform other data movement operations in parallel**
  > 
  > - **Affinitized Data Flows.** Partitions the Network Stack Processing dynamically across multiple physical or logical CPUs. This allows CPU cycles to be allocated to the application for faster execution.
  > 
  > - **Asynchronous Low-Cost Copy.** Intel Quick Data Technology provides enhanced data movement, allowing payload data copies from the NIC buffer in system memory to the application buffer with far fewer CPU cycles, returning the saved CPU cycles to  productive application workloads
  > 
  > - **Improved TCP/IP Processing with Optimized TCP/IP Stack.** Implements separate packet data and control paths to optimize  processing of the packet header from the packet payload. This and other stack-related enhancements reduce protocol processing cycles.

## 运行环境

ROS目前只能在基于Unix的平台上运行，Window版本仍在开发中。ROS的软件主要在Ubuntu和Mac OS X 系统上测试，同时ROS社区仍持续支持Fedora，Gentoo，Arch Linux和其它Linux平台。

我们计划使用较为成熟的Ubuntu作为运行ROS的操作系统，以虚拟机作为运行平台：

### QEMU

> 将小车(以及其他的实体硬件设备)作为测试实时性的载体有难度，且与操作系统课程的核心内容无关，故本小组决定在`QEMU`上跑出一段`benchmark`程序

#### What is QEMU

* `QEMU`[^12]是可执行**硬件虚拟化**(对计算机/操作系统的虚拟)的(hardware virtualization)开源仿真器(Emulator)。`QEMU`是一个托管的虚拟机，它通过动态的二进制转换，模拟CPU，并且提供一组设备模型，使它能够运行多种未修改的客户机OS，可以通过与KVM一起使用进而接近本地速度运行虚拟机(接近真实电脑的速度)。

* QEMU有多种模式
  
  - **User mode**：又称作“用户模式”，在这种模式下，QEMU运行针对不同指令编译的单个Linux或Darwin/macOS程序。系统调用与32/64位接口适应。在这种模式下，我们可以实现交叉编译（cross-compilation）与交叉侦错（cross- debugging）。
  - **System mode**：“系统模式”，在这种模式下，QEMU模拟一个完整的计算机系统，包括外围设备。它可以用于在一台计算机上提供多台虚拟计算机的虚拟主机。 QEMU可以实现许多客户机OS的引导，比如x86，MIPS，32-bit ARMv7，PowerPC等等。
  - **KVM Hosting**：QEMU在这时处理KVM镜像的设置与迁移，并参加硬件的仿真，但是客户端的执行则由KVM完成。
  - **Xen Hosting**：在这种托管下，客户端的执行几乎完全在Xen中完成，并且对QEMU屏蔽。QEMU只提供硬件仿真的支持。

* 由于`QEMU`是纯软件实现的，所有的指令都需要`QEMU`转手，因此性能很低。配合`KVM`(基于内核的虚拟机，主要负责比较繁琐的CPU、内存虚拟化)来完成虚拟化是一种常用的解决方式，`QEMU`主要负责I/O的虚拟化。
  
  ![](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/27d16702992f42a48a0bbe878d6f4485~tplv-k3u1fbpfcp-zoom-in-crop-mark:4536:0:0:0.awebp)

* `QEMU` 软件虚拟化实现的思路是采用二进制指令翻译技术，主要是提取 guest 代码，然后将其翻译成 TCG(微型代码生成器) 中间代码，最后再将中间代码翻译成 host 指定架构的代码。
  
  ![](https://pic1.zhimg.com/80/v2-5e64a6216ce55c266c7760e558d72ecc_1440w.webp)

* 本质上，虚拟出的每个虚拟机对应 host 上的一个 `QEMU` 进程，而虚拟机的执行线程（如 CPU 线程、I/O 线程等）对应` QEMU`进程的一个线程。

## 重要性分析

- 金融领域
  
  在程序化交易系统下，为什么需要关注低延迟？
  
  程序化交易系统是接收市场的行情再去进行运算，然后发出交易信号。发出交易信号越早，就越可能挣到钱，如果晚了，钱都被别人挣了，自己可能就会亏钱。所以在这种场景下，**低延迟是第一需求，不会追求吞吐量**。交易所都有流速权，即每秒的报单速度是有限的，不允许做很大的吞吐，所以金融对低延迟的要求是比较高的，也**不在意资源利用率**。因为我们的 CPU 会进行绑核，绑核会让 CPU 处于 **busy looping**，让它的占有率达到100%，那么资源利用率就没有任何参考价值。
  
  我国金融信息行业，交易所行情数据目前主要采用**STEP**（Securities Trading Exchange
  Protocol）协议，并基于国际主流金融信息交换标准的**FIX**（Financial InformationeX change protocol）协议，根据行情数据的流式处理特征，进行了**FAST**（FIX Adapted For Streaming）编码压缩[^3]。
  
  目前行情数据流的解析延时主要就来自于以上协议层的处理。现有的行情解析方案也着重于围绕通信和数据流解码两方面展开优化，主要存在以下三种主流解决方案：
  
  - 高性能CPU
  
  - 高性能CPU + 基于ASIC 的智能网卡
    
    此方案在软件层优化FAST 解码的基础上，通过采用如Solar Flare、Mellanox、Intel 等公司的专用低延时网卡，配合 kernel bypass 技术，进一步降低通信延时开销，从而达到亚毫秒级的行情解析处理。
  
  - 高性能CPU + FPGA 加速卡

- 无人驾驶安全性[^1]
  
  无人驾驶以其智能化、无人化、高效使用城市资源等特点，已经成为了未来发展的必然趋势。然而，近年来由于无人驾驶操作系统处理不及时等问题，已经造成了人员伤亡。想要发展无人驾驶技术，解决相关问题刻不容缓。因此，我们小组希望能对ROS操作系统的实时性进行优化，做到快速处理障碍物信息，快速做出刹车处理，保障人身安全。

- 无人机集群调度
  
  无人机集群调度对实时性的要求很高，因为无人机集群通常需要在复杂的环境中协同工作，例如执行搜索和救援任务、监控大型基础设施或进行军事侦察等。在这些应用场景中，无人机集群必须能够实时响应操作指令，并根据环境变化进行适时调整。此外，无人机集群还需要满足安全性和可靠性的要求，确保它们能够在不发生重大事故的情况下完成任务。因此，无人机集群调度必须具备高度的实时性和精准度。

## 相关项目

- [OSH-2019-x-monthly-subscription](https://github.com/OSH-2019/x-monthly-subscription)
  
  - Linux
  - 成果：方差分析算法的 eBPF 实现，降低处理数据流延迟


- [OSH-2022-x_NooBirds](https://github.com/OSH-2022/x-NooBirds)
  
  - 实时Linux + 树莓派
  
  - 成果：无人车位置测量、路线规划、路口防撞、防追尾
  
  - 多个物理终端实时调度
  
  - 对比多种调度算法实际效果
  
  - 改进方向：
    
    > 硬件导致的响应延时
    > 通信结构由主从变为平行
    > 离线调度

- [OSH-2022-x_do_our_best](https://github.com/OSH-2022/x_do_our_best)
  
  - 实时内核RT-Thread + ROS + stm32
  - 成果：小车复杂任务 + 多车调度

## 参考文献

[^1]:osrf/gazebo_modelsgithub.com/osrf/gazebo_models, https://link.zhihu.com/?target=https%3A//github.com/osrf/gazebo_models.
[^1]:Zaharia, Matei, et al. “Spark: Cluster Computing With Working Sets.” IEEE International Conference on Cloud Computing Technology and Science, June 2010, p. 10. www2.eecs.berkeley.edu/Pubs/TechRpts/2010/EECS-2010-53.pdf.

[^2]:零拷贝、顺序读写、堆外内存, https://blog.csdn.net/daijiguo/article/details/106457319.

[^3]:上海证券交易所交易技术前沿， http://www.sse.com.cn/services/tradingservice/tradingtech/sh/transaction/ITRDC_TechMag_041_202012.pdf.

[^4]:ROS2官方文档(中文版), http://dev.ros2.fishros.com/doc/index.html.

[^5]:ROS2_design, http://design.ros2.org/.

[^6]:ROS2调度策略, https://zhuanlan.zhihu.com/p/404067881.

[^7]:DMA维基百科, https://en.wikipedia.org/wiki/Direct_memory_access.

[^8]:cache的基本知识， https://zhuanlan.zhihu.com/p/102293437.

[^9]:cache操作， https://blog.csdn.net/guyongqiangx/article/details/52045849.

[^10]:WebRTC超低低延迟直播CDN集成规范草案， https://github.com/rtcdn/rtcdn-draft.

[^11]:DMA_API使用文档， https://www.kernel.org/doc/html/latest/core-api/dma-api-howto.html.

[^12]:QEMU介绍， https://zhuanlan.zhihu.com/p/72484589.
